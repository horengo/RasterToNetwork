{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote skeleton raster: /home/hector/Downloads/skeleton_corridors<015.tif\n"
     ]
    }
   ],
   "source": [
    "# Skeletonise a binary channel raster (multi-pixel thickness -> 1-pixel centreline)\n",
    "# Output is a georeferenced uint8 raster: 1 = skeleton, 0 = background (and preserves nodata if present)\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "# ---- USER PARAMS ----\n",
    "in_raster   = \"/home/hector/Documents/raster_file.tif\"\n",
    "out_raster  = \"/home/hector/Documents/skeleton.tif\"\n",
    "\n",
    "# How to binarize (edit if needed):\n",
    "# - if raster is 0/1 -> (arr == 1)\n",
    "# - if raster is probability 0..1 -> (arr >= 0.5)\n",
    "# - if raster is 0/255 -> (arr > 0)\n",
    "binarize = lambda arr: (arr > 0)\n",
    "# ---------------------\n",
    "\n",
    "try:\n",
    "    from skimage.morphology import skeletonize\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"This cell requires scikit-image. Install with: pip install scikit-image (or conda install scikit-image)\") from e\n",
    "\n",
    "with rasterio.open(in_raster) as src:\n",
    "    prof = src.profile.copy()\n",
    "    nodata = src.nodata\n",
    "    arr = src.read(1, masked=True)  # masked array if nodata exists\n",
    "\n",
    "# Build boolean mask for channels, respecting nodata\n",
    "binary = np.zeros(arr.shape, dtype=bool)\n",
    "valid = ~arr.mask if np.ma.isMaskedArray(arr) else np.ones(arr.shape, dtype=bool)\n",
    "binary[valid] = binarize(np.asarray(arr)[valid])\n",
    "\n",
    "# Skeletonise: 1-pixel-wide centreline\n",
    "skel = skeletonize(binary).astype(np.uint8)  # 0/1\n",
    "\n",
    "# Optionally restore nodata (so nodata isn't forced to 0)\n",
    "if nodata is not None and np.any(~valid):\n",
    "    skel = skel.astype(np.uint8)\n",
    "    # Keep as uint8 for GIS friendliness; nodata will be set in metadata below\n",
    "    skel[~valid] = 0  # keep nodata pixels as 0 in data; nodata flag handles masking in GIS\n",
    "\n",
    "# Write output\n",
    "prof.update(dtype=rasterio.uint8, count=1, compress=\"deflate\", nodata=0 if nodata is None else nodata)\n",
    "with rasterio.open(out_raster, \"w\", **prof) as dst:\n",
    "    dst.write(skel, 1)\n",
    "\n",
    "print(f\"Done. Wrote skeleton raster: {out_raster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote: /home/hector/Documents/Nazarij/filter_skeleton_50conPix.tif\n",
      "Removed components with < 50 pixels (connectivity=8).\n"
     ]
    }
   ],
   "source": [
    "# Connected-pixel filtering of a binary channel raster (remove small components)\n",
    "# - Reads a binary raster (0/1 or 0/255 etc.)\n",
    "# - Labels connected components\n",
    "# - Removes components with fewer than `min_pixels` pixels\n",
    "# - Writes a filtered binary raster with the same georeferencing\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "\n",
    "# ---- USER PARAMS ----\n",
    "in_raster   = \"/home/hector/Documents/skeleton.tif\"\n",
    "out_raster  = \"/home/hector/Documents/filter_skeleton_50conPix.tif\"\n",
    "min_pixels  = 50          # remove components smaller than this (in pixels)\n",
    "connectivity = 8          # 4 or 8 (8 is usually better for channel networks)\n",
    "\n",
    "# If your raster is not strictly 0/1, define how to binarize it:\n",
    "# e.g., channels are values > 0\n",
    "binarize = lambda arr: (arr > 0)\n",
    "\n",
    "# ---------------------\n",
    "try:\n",
    "    from scipy.ndimage import label\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"This cell requires scipy. Install it with: pip install scipy (or conda install scipy)\") from e\n",
    "\n",
    "if connectivity not in (4, 8):\n",
    "    raise ValueError(\"connectivity must be 4 or 8\")\n",
    "\n",
    "# Structuring element defines pixel connectivity\n",
    "structure = np.array([[0,1,0],\n",
    "                      [1,1,1],\n",
    "                      [0,1,0]], dtype=np.uint8) if connectivity == 4 else np.ones((3,3), dtype=np.uint8)\n",
    "\n",
    "with rasterio.open(in_raster) as src:\n",
    "    prof = src.profile.copy()\n",
    "    nodata = src.nodata\n",
    "\n",
    "    band = src.read(1, masked=True)  # masked array if nodata exists\n",
    "    # Build boolean binary mask for channels, respecting nodata\n",
    "    binary = np.zeros(band.shape, dtype=bool)\n",
    "    valid = ~band.mask if np.ma.isMaskedArray(band) else np.ones(band.shape, dtype=bool)\n",
    "    binary[valid] = binarize(np.asarray(band)[valid])\n",
    "\n",
    "    # Label connected components (only on True pixels)\n",
    "    labels, nlab = label(binary, structure=structure)\n",
    "\n",
    "    if nlab == 0:\n",
    "        # Nothing to filter; just write an empty binary raster (or original)\n",
    "        filtered = binary.astype(np.uint8)\n",
    "    else:\n",
    "        # Component sizes (labels start at 1; label 0 is background)\n",
    "        sizes = np.bincount(labels.ravel())\n",
    "        keep = sizes >= min_pixels\n",
    "        keep[0] = False  # never keep background\n",
    "\n",
    "        filtered = keep[labels].astype(np.uint8)\n",
    "\n",
    "    # If you want to preserve nodata (rather than forcing nodata to 0), apply it back:\n",
    "    if nodata is not None and np.any(~valid):\n",
    "        filtered = filtered.astype(np.uint8)\n",
    "        # set nodata pixels to nodata value (commonly 0, but not always)\n",
    "        filtered = filtered.astype(np.float32) if prof[\"dtype\"] in (\"float32\", \"float64\") else filtered\n",
    "        filtered[~valid] = nodata\n",
    "\n",
    "    # Write output: keep it binary uint8 unless you explicitly need another dtype\n",
    "    prof.update(dtype=rasterio.uint8, count=1, nodata=0 if nodata is None else nodata, compress=\"deflate\")\n",
    "    # If nodata exists and is not 0, you may prefer to keep prof[\"dtype\"] instead; adjust as needed.\n",
    "\n",
    "    with rasterio.open(out_raster, \"w\", **prof) as dst:\n",
    "        dst.write(filtered.astype(np.uint8), 1)\n",
    "\n",
    "print(f\"Done. Wrote: {out_raster}\")\n",
    "print(f\"Removed components with < {min_pixels} pixels (connectivity={connectivity}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gap filling] endpoint↔endpoint bridges added: 2533 | endpoint→vertex snaps added: 2263\n",
      "[Parallel merge] disabled\n",
      "Wrote network to: /home/hector/Documents/Nazarij/channels_50conPix_NEW3network.gpkg (layers: nodes, edges)\n",
      "Original sknw export: nodes=35890 edges=32650\n",
      "Contracted:          nodes=35007 edges=31767\n",
      "Final:              nodes=37143 edges=38699\n",
      "simplify_tol=10.93 smooth_iters=2\n"
     ]
    }
   ],
   "source": [
    "# Cell: Transform a filtered skeleton raster (0/1) into a network with:\n",
    "#  - endpoint↔endpoint gap bridging (angle + distance)\n",
    "#  - endpoint→vertex snapping (attach dangling endpoints to nearby line vertices)\n",
    "#  - degree-2 contraction (analysis network)\n",
    "#  - optional parallel/near-parallel connector creation (with proper node insertion)\n",
    "#  - optional directionality from a slope-direction/aspect raster\n",
    "#  - optional per-edge cost extraction from a cost raster (sum of traversed cells)\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import networkx as nx\n",
    "\n",
    "# ---- USER PARAMS ------------------------------------------------------------\n",
    "in_skeleton = \"/home/hector/Documents/filter_skeleton_50conPix.tif\"\n",
    "out_gpkg    = \"/home/hector/Documents/channels_50conPix_network.gpkg\"\n",
    "\n",
    "# Geometry clean-up (map units)\n",
    "simplify_tol = None          # if None, auto ~ 0.5 pixel\n",
    "smooth_iters = 2             # 0 disables smoothing; 1-3 typical\n",
    "smooth_then_simplify = True  # re-simplify after smoothing\n",
    "\n",
    "# ---- Gap filling: endpoint -> endpoint (done post-contraction) ----\n",
    "bridge_endpoints = True\n",
    "max_gap_dist = 900.0          # max distance to bridge\n",
    "max_angle_deg = 65.0          # endpoints must face each other within this angle\n",
    "direction_step = 3            # vertices ahead to estimate local direction\n",
    "max_bridges_per_endpoint = 1\n",
    "max_bridge_rounds = 2         # run bridging in multiple rounds (helps when topology changes)\n",
    "\n",
    "# ---- Gap filling: endpoint -> vertex snapping (endpoint to an *existing vertex* on another line) ----\n",
    "bridge_to_vertices = True\n",
    "max_snap_dist = 600.0          # max distance endpoint->vertex\n",
    "max_snap_angle_deg = 45.0      # endpoint extension must align within this angle\n",
    "max_target_angle_deg = 90.0    # how \"clean\" the attachment is to the target line at that vertex. 90.0 = T-type attachment\n",
    "max_vertex_snaps_per_endpoint = 1\n",
    "vertex_candidate_stride = 1    # check every vertex (1), or subsample (e.g., 2, 3) if huge dataset\n",
    "\n",
    "# ---- Optional directionality from a slope/aspect/flow-direction raster ----\n",
    "slope_dir_raster = None        # e.g. \"/path/to/aspect.tif\" or None to disable\n",
    "slope_dir_convention = \"cw_from_north\"  # \"cw_from_north\" or \"ccw_from_east\"\n",
    "slope_dir_is_downhill = True   # True if raster indicates downhill direction; False if uphill\n",
    "\n",
    "# ---- Optional costs per edge from a cost raster ----\n",
    "cost_raster = None             # e.g. \"/path/to/cost.tif\" or None to disable\n",
    "cost_nodata_to_nan = True\n",
    "cost_sampling_step = None      # if None, ~0.5 pixel; else spacing in map units for sampling points\n",
    "cost_unique_cells = True       # sum unique traversed cells (recommended)\n",
    "\n",
    "# ---- Join closely running (parallel / near-parallel) lines by adding connectors ----\n",
    "join_parallel_lines = False\n",
    "parallel_max_dist = 40.0       # max separation between lines to consider connecting\n",
    "parallel_max_angle_deg = 15.0  # max angular difference between overall directions (mod 180)\n",
    "parallel_min_length = 50.0     # ignore very short edges\n",
    "max_parallel_connectors = 200  # safety cap\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# deps\n",
    "try:\n",
    "    import sknw  # pip install sknw\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Install sknw with: pip install sknw\") from e\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point, LineString\n",
    "    from shapely.strtree import STRtree\n",
    "    from shapely.ops import substring, nearest_points\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Install geopandas + shapely with: pip install geopandas shapely\") from e\n",
    "\n",
    "\n",
    "# ---------------------- numeric helpers -------------------------------------\n",
    "def angle_deg(v1, v2):\n",
    "    v1 = np.asarray(v1, dtype=float); v2 = np.asarray(v2, dtype=float)\n",
    "    n1 = np.linalg.norm(v1); n2 = np.linalg.norm(v2)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return 180.0\n",
    "    cosang = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosang)))\n",
    "\n",
    "\n",
    "def angle_diff_180(a_deg, b_deg):\n",
    "    \"\"\"Smallest angular difference modulo 180 degrees (for undirected parallel comparison).\"\"\"\n",
    "    d = abs((a_deg - b_deg) % 180.0)\n",
    "    return min(d, 180.0 - d)\n",
    "\n",
    "\n",
    "def bearing_cw_from_north(p0, p1):\n",
    "    \"\"\"Bearing degrees clockwise from North for (x,y) with +y = North.\"\"\"\n",
    "    p0 = np.asarray(p0, float); p1 = np.asarray(p1, float)\n",
    "    dx = p1[0] - p0[0]\n",
    "    dy = p1[1] - p0[1]\n",
    "    ang = np.degrees(np.arctan2(dx, dy))  # atan2(x,y) => cw from north\n",
    "    return float((ang + 360.0) % 360.0)\n",
    "\n",
    "\n",
    "def convert_slope_dir_to_cw_from_north(val_deg, convention):\n",
    "    \"\"\"Convert raster direction to cw-from-north degrees.\"\"\"\n",
    "    if not np.isfinite(val_deg):\n",
    "        return np.nan\n",
    "    v = float(val_deg)\n",
    "    if convention == \"cw_from_north\":\n",
    "        return (v % 360.0)\n",
    "    elif convention == \"ccw_from_east\":\n",
    "        # 0=east ccw -> 90=north; convert to cw-from-north: cw = (90 - ccw_east) mod 360\n",
    "        return float((90.0 - v) % 360.0)\n",
    "    else:\n",
    "        raise ValueError(\"slope_dir_convention must be 'cw_from_north' or 'ccw_from_east'\")\n",
    "\n",
    "\n",
    "def pixel_size_from_transform(transform):\n",
    "    px = np.hypot(transform.a, transform.b)\n",
    "    py = np.hypot(transform.d, transform.e)\n",
    "    return float((px + py) / 2.0)\n",
    "\n",
    "\n",
    "# ---------------------- raster/geometry helpers -----------------------------\n",
    "def pix_to_xy(transform, rc):\n",
    "    rows = rc[:, 0].astype(int)\n",
    "    cols = rc[:, 1].astype(int)\n",
    "    xs, ys = rasterio.transform.xy(transform, rows, cols, offset=\"center\")\n",
    "    return np.column_stack([np.asarray(xs), np.asarray(ys)])\n",
    "\n",
    "\n",
    "def node_xy(G, n, transform):\n",
    "    rc = np.array(G.nodes[n][\"o\"], dtype=float).reshape(1, 2)\n",
    "    return pix_to_xy(transform, rc)[0]\n",
    "\n",
    "\n",
    "def snap_edge_endpoints_to_nodes(G, u, v, k, transform):\n",
    "    \"\"\"\n",
    "    Edge coords oriented u->v, and endpoints snapped EXACTLY to node coords.\n",
    "    \"\"\"\n",
    "    pts_rc = G.edges[u, v, k][\"pts\"]\n",
    "    xy = pix_to_xy(transform, pts_rc)\n",
    "\n",
    "    xu = node_xy(G, u, transform)\n",
    "    xv = node_xy(G, v, transform)\n",
    "\n",
    "    # orient so start closer to u\n",
    "    if np.linalg.norm(xy[0] - xu) <= np.linalg.norm(xy[-1] - xu):\n",
    "        xy_oriented = xy.copy()\n",
    "    else:\n",
    "        xy_oriented = xy[::-1].copy()\n",
    "\n",
    "    xy_oriented[0]  = xu\n",
    "    xy_oriented[-1] = xv\n",
    "    return xy_oriented\n",
    "\n",
    "\n",
    "def chaikin_smooth(coords, n_iter=2):\n",
    "    \"\"\"Chaikin smoothing, keeping endpoints.\"\"\"\n",
    "    if n_iter <= 0 or len(coords) < 3:\n",
    "        return coords\n",
    "    out = coords\n",
    "    for _ in range(n_iter):\n",
    "        P = out\n",
    "        Q = 0.75 * P[:-1] + 0.25 * P[1:]\n",
    "        R = 0.25 * P[:-1] + 0.75 * P[1:]\n",
    "        out2 = np.vstack([P[0], np.column_stack([Q, R]).reshape(-1, 2), P[-1]])\n",
    "        _, idx = np.unique(out2, axis=0, return_index=True)\n",
    "        out = out2[np.sort(idx)]\n",
    "        if len(out) < 3:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def simplify_coords(coords, tol):\n",
    "    \"\"\"Shapely simplify; preserve endpoints.\"\"\"\n",
    "    if tol is None or tol <= 0 or len(coords) < 3:\n",
    "        return coords\n",
    "    ls = LineString(coords)\n",
    "    ls2 = ls.simplify(float(tol), preserve_topology=True)\n",
    "    c = np.asarray(ls2.coords)\n",
    "    c[0] = coords[0]\n",
    "    c[-1] = coords[-1]\n",
    "    return c\n",
    "\n",
    "\n",
    "def split_linestring_at_point(line, pt, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Split LineString into two at projection of pt onto line.\n",
    "    Returns (seg1, seg2, at_endpoint_bool).\n",
    "    \"\"\"\n",
    "    if line.is_empty or line.length == 0:\n",
    "        return line, None, True\n",
    "    d = float(line.project(pt))\n",
    "    L = float(line.length)\n",
    "    if d <= eps or d >= L - eps:\n",
    "        return line, None, True\n",
    "    seg1 = substring(line, 0.0, d)\n",
    "    seg2 = substring(line, d, L)\n",
    "    return seg1, seg2, False\n",
    "\n",
    "\n",
    "def endpoint_direction_from_linestring(endpoint_xy, line_coords, step=3):\n",
    "    \"\"\"\n",
    "    line_coords is oriented so that coords[0] is endpoint (or very close).\n",
    "    returns vector from endpoint into the line interior.\n",
    "    \"\"\"\n",
    "    if len(line_coords) < 2:\n",
    "        return np.array([0.0, 0.0])\n",
    "    i1 = min(int(step), len(line_coords) - 1)\n",
    "    return np.asarray(line_coords[i1], float) - np.asarray(line_coords[0], float)\n",
    "\n",
    "\n",
    "def raster_value_at_xy(src, xy):\n",
    "    try:\n",
    "        val = next(src.sample([(float(xy[0]), float(xy[1]))]))[0]\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    if val is None:\n",
    "        return np.nan\n",
    "    try:\n",
    "        if np.ma.isMaskedArray(val) and val.mask:\n",
    "            return np.nan\n",
    "    except Exception:\n",
    "        pass\n",
    "    return float(val)\n",
    "\n",
    "\n",
    "def cost_sum_along_line(line, cost_arr, transform, nodata, step, nodata_to_nan=True, unique_cells=True):\n",
    "    \"\"\"\n",
    "    Approximate sum of raster cost along a line.\n",
    "    - If unique_cells=True: sample points, map to cells, sum unique traversed cells.\n",
    "    - Else: sum values at sample points (can double count cells).\n",
    "    \"\"\"\n",
    "    if line.is_empty or line.length == 0:\n",
    "        return 0.0\n",
    "\n",
    "    L = float(line.length)\n",
    "    n = max(2, int(np.ceil(L / float(step))) + 1)\n",
    "    ds = np.linspace(ளம் := 0.0, L, n)  # keep deterministic\n",
    "\n",
    "    pts = [line.interpolate(float(d)) for d in ds]\n",
    "    xs = np.array([p.x for p in pts], dtype=float)\n",
    "    ys = np.array([p.y for p in pts], dtype=float)\n",
    "\n",
    "    rows, cols = rasterio.transform.rowcol(transform, xs, ys)\n",
    "    rc = np.column_stack([rows, cols]).astype(int)\n",
    "\n",
    "    h, w = cost_arr.shape\n",
    "    ok = (rc[:, 0] >= 0) & (rc[:, 0] < h) & (rc[:, 1] >= 0) & (rc[:, 1] < w)\n",
    "    rc = rc[ok]\n",
    "    if rc.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if unique_cells:\n",
    "        rc = np.unique(rc, axis=0)\n",
    "\n",
    "    vals = cost_arr[rc[:, 0], rc[:, 1]].astype(float)\n",
    "\n",
    "    if nodata is not None:\n",
    "        m = (vals == nodata)\n",
    "        if m.any():\n",
    "            if nodata_to_nan:\n",
    "                vals[m] = np.nan\n",
    "            else:\n",
    "                vals[m] = 0.0\n",
    "\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if vals.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.sum(vals))\n",
    "\n",
    "\n",
    "# ---------------------- export from sknw graph ------------------------------\n",
    "def build_nodes_edges_gdfs(G, transform, crs, simplify_tol, smooth_iters):\n",
    "    # nodes\n",
    "    node_rows = []\n",
    "    node_xy_cache = {}\n",
    "    for n in G.nodes:\n",
    "        xy = node_xy(G, n, transform)\n",
    "        node_xy_cache[n] = xy\n",
    "        node_rows.append({\n",
    "            \"node\": int(n),\n",
    "            \"degree\": int(G.degree[n]),\n",
    "            \"x\": float(xy[0]),\n",
    "            \"y\": float(xy[1]),\n",
    "            \"geometry\": Point(xy)\n",
    "        })\n",
    "    nodes_gdf = gpd.GeoDataFrame(node_rows, geometry=\"geometry\", crs=crs)\n",
    "\n",
    "    # edges\n",
    "    edge_rows = []\n",
    "    for u, v, k, data in G.edges(keys=True, data=True):\n",
    "        xy = snap_edge_endpoints_to_nodes(G, u, v, k, transform)\n",
    "\n",
    "        xy = simplify_coords(xy, simplify_tol)\n",
    "        if smooth_iters > 0:\n",
    "            xy = chaikin_smooth(xy, n_iter=smooth_iters)\n",
    "            if smooth_then_simplify:\n",
    "                xy = simplify_coords(xy, simplify_tol)\n",
    "\n",
    "        geom = LineString(xy)\n",
    "        edge_rows.append({\n",
    "            \"u\": int(u), \"v\": int(v), \"k\": int(k),\n",
    "            \"n_vert\": int(len(xy)),\n",
    "            \"length\": float(geom.length),\n",
    "            \"is_bridge\": bool(data.get(\"is_bridge\", False)),\n",
    "            \"bridge_type\": str(data.get(\"bridge_type\", \"\")) if data.get(\"is_bridge\", False) else \"\",\n",
    "            \"geometry\": geom\n",
    "        })\n",
    "    edges_gdf = gpd.GeoDataFrame(edge_rows, geometry=\"geometry\", crs=crs)\n",
    "    return nodes_gdf, edges_gdf\n",
    "\n",
    "\n",
    "# ---------------------- degree-2 contraction --------------------------------\n",
    "def contract_degree2_graph(G, nodes_gdf, edges_gdf):\n",
    "    \"\"\"\n",
    "    Contract degree-2 nodes by merging chains into single edges between 'kept' nodes.\n",
    "    \"\"\"\n",
    "    node_geom = {int(r.node): r.geometry for r in nodes_gdf.itertuples(index=False)}\n",
    "    node_xy_  = {int(r.node): np.array([float(r.x), float(r.y)]) for r in nodes_gdf.itertuples(index=False)}\n",
    "    deg = dict(G.degree)\n",
    "\n",
    "    keep = {int(n) for n in G.nodes if deg[n] != 2}\n",
    "\n",
    "    # pure cycles: keep one node per all-degree-2 component\n",
    "    for comp in nx.connected_components(nx.Graph(G)):\n",
    "        comp = {int(n) for n in comp}\n",
    "        if comp and all(deg[n] == 2 for n in comp):\n",
    "            keep.add(min(comp))\n",
    "\n",
    "    coords_map = {}\n",
    "    for r in edges_gdf.itertuples(index=False):\n",
    "        coords_map[(int(r.u), int(r.v), int(r.k))] = np.asarray(r.geometry.coords, dtype=float)\n",
    "\n",
    "    def norm_eid(a, b, k):\n",
    "        a = int(a); b = int(b); k = int(k)\n",
    "        return (a, b, k) if a <= b else (b, a, k)\n",
    "\n",
    "    def get_seg_coords(a, b, k):\n",
    "        a = int(a); b = int(b); k = int(k)\n",
    "        if (a, b, k) in coords_map:\n",
    "            return coords_map[(a, b, k)]\n",
    "        if (b, a, k) in coords_map:\n",
    "            return coords_map[(b, a, k)][::-1]\n",
    "        return np.vstack([node_xy_[a], node_xy_[b]])\n",
    "\n",
    "    visited = set()\n",
    "    out_edges = []\n",
    "\n",
    "    for s in sorted(keep):\n",
    "        for nbr, keydict in G[s].items():\n",
    "            for k in keydict.keys():\n",
    "                eid = norm_eid(s, nbr, k)\n",
    "                if eid in visited:\n",
    "                    continue\n",
    "\n",
    "                prev = int(s)\n",
    "                curr = int(nbr)\n",
    "                kk   = int(k)\n",
    "\n",
    "                chain_coords = list(get_seg_coords(prev, curr, kk))\n",
    "                visited.add(eid)\n",
    "                chain_n_segs = 1\n",
    "\n",
    "                while curr not in keep and deg[curr] == 2:\n",
    "                    found = False\n",
    "                    for nbr2, keydict2 in G[curr].items():\n",
    "                        nbr2 = int(nbr2)\n",
    "                        if nbr2 == prev:\n",
    "                            continue\n",
    "                        for k2 in keydict2.keys():\n",
    "                            k2 = int(k2)\n",
    "                            eid2 = norm_eid(curr, nbr2, k2)\n",
    "                            if eid2 in visited:\n",
    "                                continue\n",
    "                            seg = list(get_seg_coords(curr, nbr2, k2))\n",
    "                            chain_coords.extend(seg[1:])\n",
    "                            visited.add(eid2)\n",
    "                            chain_n_segs += 1\n",
    "                            prev, curr = curr, nbr2\n",
    "                            found = True\n",
    "                            break\n",
    "                        if found:\n",
    "                            break\n",
    "                    if not found:\n",
    "                        break\n",
    "\n",
    "                t = int(curr)\n",
    "                if len(chain_coords) >= 2:\n",
    "                    geom = LineString(chain_coords)\n",
    "                    out_edges.append({\n",
    "                        \"u\": int(s),\n",
    "                        \"v\": int(t),\n",
    "                        \"n_segs\": int(chain_n_segs),\n",
    "                        \"length\": float(geom.length),\n",
    "                        \"is_bridge\": False,\n",
    "                        \"bridge_type\": \"\",\n",
    "                        \"geometry\": geom\n",
    "                    })\n",
    "\n",
    "    edges2_gdf = gpd.GeoDataFrame(out_edges, geometry=\"geometry\", crs=edges_gdf.crs)\n",
    "\n",
    "    # degrees in contracted graph\n",
    "    H = nx.Graph()\n",
    "    for r in edges2_gdf.itertuples(index=False):\n",
    "        H.add_edge(int(r.u), int(r.v))\n",
    "\n",
    "    node_rows = []\n",
    "    for n in sorted(keep):\n",
    "        if n not in node_geom:\n",
    "            continue\n",
    "        xy = node_xy_[n]\n",
    "        node_rows.append({\n",
    "            \"node\": int(n),\n",
    "            \"degree\": int(H.degree[n]) if n in H else 0,\n",
    "            \"x\": float(xy[0]),\n",
    "            \"y\": float(xy[1]),\n",
    "            \"geometry\": node_geom[n]\n",
    "        })\n",
    "    nodes2_gdf = gpd.GeoDataFrame(node_rows, geometry=\"geometry\", crs=nodes_gdf.crs)\n",
    "    return nodes2_gdf, edges2_gdf\n",
    "\n",
    "\n",
    "# ---------------------- mutable network utilities ---------------------------\n",
    "def recompute_node_degrees(nodes, edges):\n",
    "    deg = {int(n): 0 for n in nodes.keys()}\n",
    "    for e in edges:\n",
    "        deg[int(e[\"u\"])] = deg.get(int(e[\"u\"]), 0) + 1\n",
    "        deg[int(e[\"v\"])] = deg.get(int(e[\"v\"]), 0) + 1\n",
    "    return deg\n",
    "\n",
    "\n",
    "def build_node_dict(nodes_gdf):\n",
    "    nodes = {}\n",
    "    for r in nodes_gdf.itertuples(index=False):\n",
    "        nodes[int(r.node)] = np.array([float(r.x), float(r.y)], dtype=float)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def make_or_get_node_at_point(nodes, pt, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Create/reuse node at pt; reuse if within tol of an existing node.\n",
    "    \"\"\"\n",
    "    xy = np.array([float(pt.x), float(pt.y)], dtype=float)\n",
    "    # quick reuse: exact match by rounding\n",
    "    key = (round(xy[0], 6), round(xy[1], 6))\n",
    "    # map may not exist; build lazily\n",
    "    return xy, key\n",
    "\n",
    "\n",
    "def build_rounding_index(nodes, nd=6):\n",
    "    idx = {}\n",
    "    for nid, xy in nodes.items():\n",
    "        idx[(round(float(xy[0]), nd), round(float(xy[1]), nd))] = int(nid)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def ensure_node(nodes, idx, xy, next_id):\n",
    "    key = (round(float(xy[0]), 6), round(float(xy[1]), 6))\n",
    "    if key in idx:\n",
    "        return idx[key], next_id\n",
    "    nid = int(next_id)\n",
    "    next_id += 1\n",
    "    nodes[nid] = np.array([float(xy[0]), float(xy[1])], dtype=float)\n",
    "    idx[key] = nid\n",
    "    return nid, next_id\n",
    "\n",
    "\n",
    "def edge_coords_oriented_from_node(edge_geom, node_xy):\n",
    "    coords = np.asarray(edge_geom.coords, dtype=float)\n",
    "    if np.linalg.norm(coords[0] - node_xy) <= np.linalg.norm(coords[-1] - node_xy):\n",
    "        return coords\n",
    "    return coords[::-1].copy()\n",
    "\n",
    "\n",
    "def edge_bearing_mod180(edge_geom):\n",
    "    coords = np.asarray(edge_geom.coords, dtype=float)\n",
    "    b = bearing_cw_from_north(coords[0], coords[-1])\n",
    "    return float(b % 180.0)\n",
    "\n",
    "\n",
    "def split_edge_at_point(nodes, idx, edges, edge_i, pt, next_node_id,\n",
    "                        cost_ctx=None, parent_cost=None, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Split edges[edge_i] at pt (projected), inserting a node if interior.\n",
    "    Returns: (node_id_at_split, next_node_id, did_split_bool)\n",
    "    \"\"\"\n",
    "    e = edges[edge_i]\n",
    "    geom = e[\"geometry\"]\n",
    "    seg1, seg2, at_endpoint = split_linestring_at_point(geom, pt, eps=eps)\n",
    "\n",
    "    if at_endpoint or seg2 is None:\n",
    "        # snap to closest endpoint node id (existing)\n",
    "        xy = np.array([pt.x, pt.y], float)\n",
    "        uxy = nodes[int(e[\"u\"])]\n",
    "        vxy = nodes[int(e[\"v\"])]\n",
    "        if np.linalg.norm(uxy - xy) <= np.linalg.norm(vxy - xy):\n",
    "            return int(e[\"u\"]), next_node_id, False\n",
    "        return int(e[\"v\"]), next_node_id, False\n",
    "\n",
    "    # interior split: create/reuse node at split point\n",
    "    split_xy = np.array([pt.x, pt.y], dtype=float)\n",
    "    nid, next_node_id = ensure_node(nodes, idx, split_xy, next_node_id)\n",
    "\n",
    "    # replace edge with seg1 and append seg2\n",
    "    u = int(e[\"u\"]); v = int(e[\"v\"])\n",
    "    attrs = {k: e[k] for k in e.keys() if k not in (\"geometry\", \"u\", \"v\", \"length\", \"cost_sum\")}\n",
    "    # first piece\n",
    "    e1 = {\n",
    "        **attrs,\n",
    "        \"u\": u, \"v\": nid,\n",
    "        \"geometry\": seg1,\n",
    "        \"length\": float(seg1.length),\n",
    "    }\n",
    "    # second piece\n",
    "    e2 = {\n",
    "        **attrs,\n",
    "        \"u\": nid, \"v\": v,\n",
    "        \"geometry\": seg2,\n",
    "        \"length\": float(seg2.length),\n",
    "    }\n",
    "\n",
    "    if cost_ctx is not None:\n",
    "        e1[\"cost_sum\"] = cost_sum_along_line(seg1, cost_ctx[\"arr\"], cost_ctx[\"transform\"], cost_ctx[\"nodata\"],\n",
    "                                             cost_ctx[\"step\"], nodata_to_nan=cost_ctx[\"nodata_to_nan\"],\n",
    "                                             unique_cells=cost_ctx[\"unique_cells\"])\n",
    "        e2[\"cost_sum\"] = cost_sum_along_line(seg2, cost_ctx[\"arr\"], cost_ctx[\"transform\"], cost_ctx[\"nodata\"],\n",
    "                                             cost_ctx[\"step\"], nodata_to_nan=cost_ctx[\"nodata_to_nan\"],\n",
    "                                             unique_cells=cost_ctx[\"unique_cells\"])\n",
    "    elif \"cost_sum\" in e:\n",
    "        # if you had a cost but no ctx, carry proportionally (fallback)\n",
    "        pc = float(e.get(\"cost_sum\", 0.0))\n",
    "        L = float(geom.length) if geom.length else 1.0\n",
    "        e1[\"cost_sum\"] = pc * (float(seg1.length) / L)\n",
    "        e2[\"cost_sum\"] = pc * (float(seg2.length) / L)\n",
    "\n",
    "    edges[edge_i] = e1\n",
    "    edges.append(e2)\n",
    "    return int(nid), next_node_id, True\n",
    "\n",
    "\n",
    "# ---------------------- post-contraction: endpoint↔endpoint bridging ----------\n",
    "def build_endpoint_directions(nodes, edges, direction_step=3):\n",
    "    \"\"\"\n",
    "    Compute for degree-1 nodes:\n",
    "      - outward direction vector (pointing OUT of the endpoint) = - (into-line vector)\n",
    "    Returns: endpoints list, dir_map[endpoint_id] = outward_vector (not normalized)\n",
    "    \"\"\"\n",
    "    deg = {nid: 0 for nid in nodes.keys()}\n",
    "    for e in edges:\n",
    "        deg[int(e[\"u\"])] = deg.get(int(e[\"u\"]), 0) + 1\n",
    "        deg[int(e[\"v\"])] = deg.get(int(e[\"v\"]), 0) + 1\n",
    "\n",
    "    endpoints = [int(n) for n, d in deg.items() if d == 1]\n",
    "    dir_map = {}\n",
    "\n",
    "    # build adjacency: endpoint -> (edge index)\n",
    "    incident = {int(n): [] for n in nodes.keys()}\n",
    "    for i, e in enumerate(edges):\n",
    "        incident[int(e[\"u\"])].append(i)\n",
    "        incident[int(e[\"v\"])].append(i)\n",
    "\n",
    "    for n in endpoints:\n",
    "        if len(incident[n]) != 1:\n",
    "            continue\n",
    "        ei = incident[n][0]\n",
    "        e = edges[ei]\n",
    "        nxy = nodes[n]\n",
    "        coords = edge_coords_oriented_from_node(e[\"geometry\"], nxy)\n",
    "        d_in = endpoint_direction_from_linestring(nxy, coords, step=direction_step)\n",
    "        d_out = -d_in\n",
    "        dir_map[n] = d_out\n",
    "\n",
    "    return endpoints, dir_map\n",
    "\n",
    "\n",
    "def bridge_endpoints_round(nodes, edges, max_gap_dist, max_angle_deg, direction_step,\n",
    "                           max_bridges_per_endpoint, bridge_type=\"endpoint_endpoint\"):\n",
    "    endpoints, out_dir = build_endpoint_directions(nodes, edges, direction_step=direction_step)\n",
    "    used = {n: 0 for n in endpoints}\n",
    "    added = 0\n",
    "\n",
    "    # naive O(E^2) over endpoints; ok for moderate endpoint counts\n",
    "    for a in endpoints:\n",
    "        if used[a] >= max_bridges_per_endpoint:\n",
    "            continue\n",
    "        if a not in out_dir:\n",
    "            continue\n",
    "        xa = nodes[a]\n",
    "        da = out_dir[a]\n",
    "\n",
    "        best_b = None\n",
    "        best_score = None\n",
    "\n",
    "        for b in endpoints:\n",
    "            if b == a:\n",
    "                continue\n",
    "            if used[b] >= max_bridges_per_endpoint:\n",
    "                continue\n",
    "            if b not in out_dir:\n",
    "                continue\n",
    "\n",
    "            xb = nodes[b]\n",
    "            ab = xb - xa\n",
    "            dist = float(np.linalg.norm(ab))\n",
    "            if dist == 0 or dist > float(max_gap_dist):\n",
    "                continue\n",
    "\n",
    "            ang_a = angle_deg(da, ab)\n",
    "            db = out_dir[b]\n",
    "            ang_b = angle_deg(db, -ab)\n",
    "\n",
    "            if ang_a <= float(max_angle_deg) and ang_b <= float(max_angle_deg):\n",
    "                score = dist + 0.2 * (ang_a + ang_b)\n",
    "                if best_score is None or score < best_score:\n",
    "                    best_score = score\n",
    "                    best_b = b\n",
    "\n",
    "        if best_b is not None:\n",
    "            b = best_b\n",
    "            geom = LineString([tuple(nodes[a]), tuple(nodes[b])])\n",
    "            edges.append({\n",
    "                \"u\": int(a),\n",
    "                \"v\": int(b),\n",
    "                \"length\": float(geom.length),\n",
    "                \"n_segs\": 1,\n",
    "                \"is_bridge\": True,\n",
    "                \"bridge_type\": str(bridge_type),\n",
    "                \"geometry\": geom\n",
    "            })\n",
    "            used[a] += 1\n",
    "            used[b] += 1\n",
    "            added += 1\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "# ---------------------- post-contraction: endpoint→vertex snapping -----------\n",
    "def build_vertex_index_for_edges(edges, stride=1):\n",
    "    \"\"\"\n",
    "    Build STRtree over interior vertices of edges.\n",
    "    Returns: (tree, meta, sig_to_i)\n",
    "      meta[i] = {\"edge_i\": int, \"vidx\": int, \"xy\": np.array, \"tangent\": np.array}\n",
    "      sig_to_i maps (round(x,6), round(y,6)) -> meta index (for Shapely 1.x geometry-return cases)\n",
    "    \"\"\"\n",
    "    pts = []\n",
    "    meta = []\n",
    "    stride = max(1, int(stride))\n",
    "\n",
    "    for ei, e in enumerate(edges):\n",
    "        geom = e[\"geometry\"]\n",
    "        coords = np.asarray(geom.coords, dtype=float)\n",
    "        if len(coords) < 3:\n",
    "            continue\n",
    "        for j in range(1, len(coords) - 1, stride):\n",
    "            p = coords[j]\n",
    "            tan = coords[j + 1] - coords[j - 1]\n",
    "            g = Point(float(p[0]), float(p[1]))\n",
    "            pts.append(g)\n",
    "            meta.append({\"edge_i\": int(ei), \"vidx\": int(j), \"xy\": p.copy(), \"tangent\": tan.copy()})\n",
    "\n",
    "    if not pts:\n",
    "        return None, [], {}\n",
    "\n",
    "    tree = STRtree(pts)\n",
    "    sig_to_i = {(round(p.x, 6), round(p.y, 6)): i for i, p in enumerate(pts)}\n",
    "    return tree, meta, sig_to_i\n",
    "\n",
    "\n",
    "def snap_endpoints_to_vertices_round(nodes, edges, next_node_id, cost_ctx,\n",
    "                                    max_snap_dist, max_snap_angle_deg, max_target_angle_deg,\n",
    "                                    direction_step, max_snaps_per_endpoint, vertex_stride):\n",
    "    \"\"\"\n",
    "    Endpoint -> interior vertex snapping with Shapely 1/2 STRtree compatibility.\n",
    "    \"\"\"\n",
    "    endpoints, out_dir = build_endpoint_directions(nodes, edges, direction_step=direction_step)\n",
    "\n",
    "    tree, meta, sig_to_i = build_vertex_index_for_edges(edges, stride=vertex_stride)\n",
    "    if tree is None:\n",
    "        return 0, next_node_id\n",
    "\n",
    "    idx = build_rounding_index(nodes, nd=6)\n",
    "    used = {n: 0 for n in endpoints}\n",
    "    added = 0\n",
    "\n",
    "    for a in endpoints:\n",
    "        if used[a] >= int(max_snaps_per_endpoint):\n",
    "            continue\n",
    "        if a not in out_dir:\n",
    "            continue\n",
    "\n",
    "        xa = nodes[a]\n",
    "        da = out_dir[a]\n",
    "\n",
    "        # IMPORTANT: initialize for each endpoint\n",
    "        best = None\n",
    "        best_score = None\n",
    "\n",
    "        search_geom = Point(float(xa[0]), float(xa[1])).buffer(float(max_snap_dist))\n",
    "        hits = tree.query(search_geom)\n",
    "\n",
    "        for h in hits:\n",
    "            # Shapely 2 often returns integer indices; Shapely 1 returns geometries\n",
    "            if isinstance(h, (int, np.integer)):\n",
    "                mi = int(h)\n",
    "            else:\n",
    "                mi = sig_to_i.get((round(h.x, 6), round(h.y, 6)), None)\n",
    "\n",
    "            if mi is None or mi < 0 or mi >= len(meta):\n",
    "                continue\n",
    "\n",
    "            m = meta[mi]\n",
    "            ei = int(m[\"edge_i\"])\n",
    "            vxy = np.asarray(m[\"xy\"], dtype=float)\n",
    "\n",
    "            ab = vxy - xa\n",
    "            dist = float(np.linalg.norm(ab))\n",
    "            if dist == 0 or dist > float(max_snap_dist):\n",
    "                continue\n",
    "\n",
    "            ang_ext = angle_deg(da, ab)\n",
    "            if ang_ext > float(max_snap_angle_deg):\n",
    "                continue\n",
    "\n",
    "            tan = np.asarray(m[\"tangent\"], dtype=float)\n",
    "            ang_target = min(angle_deg(tan, -ab), angle_deg(-tan, -ab))\n",
    "            if ang_target > float(max_target_angle_deg):\n",
    "                continue\n",
    "\n",
    "            score = dist + 0.2 * ang_ext + 0.1 * ang_target\n",
    "            if best_score is None or score < best_score:\n",
    "                best_score = score\n",
    "                best = (ei, Point(float(vxy[0]), float(vxy[1])))\n",
    "\n",
    "        if best is None:\n",
    "            continue\n",
    "\n",
    "        target_ei, vpt = best\n",
    "\n",
    "        parent_cost = edges[target_ei].get(\"cost_sum\", None)\n",
    "        nid, next_node_id, did_split = split_edge_at_point(\n",
    "            nodes, idx, edges, target_ei, vpt, next_node_id,\n",
    "            cost_ctx=cost_ctx, parent_cost=parent_cost\n",
    "        )\n",
    "\n",
    "        geom = LineString([tuple(nodes[a]), tuple(nodes[nid])])\n",
    "        conn = {\n",
    "            \"u\": int(a),\n",
    "            \"v\": int(nid),\n",
    "            \"length\": float(geom.length),\n",
    "            \"n_segs\": 1,\n",
    "            \"is_bridge\": True,\n",
    "            \"bridge_type\": \"endpoint_vertex\",\n",
    "            \"geometry\": geom\n",
    "        }\n",
    "        if cost_ctx is not None:\n",
    "            conn[\"cost_sum\"] = cost_sum_along_line(\n",
    "                geom, cost_ctx[\"arr\"], cost_ctx[\"transform\"], cost_ctx[\"nodata\"], cost_ctx[\"step\"],\n",
    "                nodata_to_nan=cost_ctx[\"nodata_to_nan\"], unique_cells=cost_ctx[\"unique_cells\"]\n",
    "            )\n",
    "\n",
    "        edges.append(conn)\n",
    "        used[a] += 1\n",
    "        added += 1\n",
    "\n",
    "    return added, next_node_id\n",
    "\n",
    "\n",
    "# ---------------------- join parallel lines by adding connectors -------------\n",
    "def _edge_signature(e):\n",
    "    \"\"\"Stable-ish signature for STRtree geometry->edge lookup (handles copies).\"\"\"\n",
    "    g = e[\"geometry\"]\n",
    "    c = np.asarray(g.coords, float)\n",
    "    return (\n",
    "        round(c[0,0], 6), round(c[0,1], 6),\n",
    "        round(c[-1,0], 6), round(c[-1,1], 6),\n",
    "        round(float(g.length), 6)\n",
    "    )\n",
    "\n",
    "\n",
    "def _snap_edge_endpoints_to_node_coords(nodes, e):\n",
    "    \"\"\"Force edge geometry endpoints to coincide with node coordinates of e['u'], e['v'].\"\"\"\n",
    "    g = e[\"geometry\"]\n",
    "    coords = np.asarray(g.coords, float)\n",
    "    uxy = nodes[int(e[\"u\"])]\n",
    "    vxy = nodes[int(e[\"v\"])]\n",
    "\n",
    "    # decide which end is u by proximity\n",
    "    if np.linalg.norm(coords[0] - uxy) <= np.linalg.norm(coords[-1] - uxy):\n",
    "        coords[0] = uxy\n",
    "        coords[-1] = vxy\n",
    "    else:\n",
    "        coords[0] = vxy\n",
    "        coords[-1] = uxy\n",
    "        # also swap u/v to keep consistent orientation with geometry (optional)\n",
    "        e[\"u\"], e[\"v\"] = int(e[\"v\"]), int(e[\"u\"])\n",
    "\n",
    "    e[\"geometry\"] = LineString(coords)\n",
    "    e[\"length\"] = float(e[\"geometry\"].length)\n",
    "\n",
    "\n",
    "def _reroute_incident_edges(nodes, edges, old_node, new_node, exclude_edge_idx=None, cost_ctx=None):\n",
    "    \"\"\"\n",
    "    Rewire all edges incident to old_node to use new_node instead, updating geometries so endpoints coincide.\n",
    "    \"\"\"\n",
    "    old_node = int(old_node); new_node = int(new_node)\n",
    "    old_xy = nodes[old_node]\n",
    "    new_xy = nodes[new_node]\n",
    "\n",
    "    for i, e in enumerate(edges):\n",
    "        if e is None or e.get(\"_deleted\", False):\n",
    "            continue\n",
    "        if exclude_edge_idx is not None and i == exclude_edge_idx:\n",
    "            continue\n",
    "\n",
    "        touched = False\n",
    "        if int(e[\"u\"]) == old_node:\n",
    "            e[\"u\"] = new_node\n",
    "            touched = True\n",
    "        if int(e[\"v\"]) == old_node:\n",
    "            e[\"v\"] = new_node\n",
    "            touched = True\n",
    "        if not touched:\n",
    "            continue\n",
    "\n",
    "        # update geometry endpoint closest to old_xy to new_xy\n",
    "        coords = np.asarray(e[\"geometry\"].coords, float)\n",
    "        d0 = np.linalg.norm(coords[0] - old_xy)\n",
    "        d1 = np.linalg.norm(coords[-1] - old_xy)\n",
    "        if d0 <= d1:\n",
    "            coords[0] = new_xy\n",
    "        else:\n",
    "            coords[-1] = new_xy\n",
    "        e[\"geometry\"] = LineString(coords)\n",
    "        e[\"length\"] = float(e[\"geometry\"].length)\n",
    "\n",
    "        # if costs are enabled, recompute edge cost (safe; preserves correctness)\n",
    "        if cost_ctx is not None:\n",
    "            e[\"cost_sum\"] = cost_sum_along_line(\n",
    "                e[\"geometry\"], cost_ctx[\"arr\"], cost_ctx[\"transform\"], cost_ctx[\"nodata\"], cost_ctx[\"step\"],\n",
    "                nodata_to_nan=cost_ctx[\"nodata_to_nan\"], unique_cells=cost_ctx[\"unique_cells\"]\n",
    "            )\n",
    "\n",
    "\n",
    "def _endpoint_overlap_ratio_on_target(target_line, other_line):\n",
    "    \"\"\"\n",
    "    How much of other_line is 'covered' along target_line by projecting other endpoints onto target.\n",
    "    Used to avoid merging lines that are just locally close.\n",
    "    \"\"\"\n",
    "    if target_line.length == 0 or other_line.length == 0:\n",
    "        return 0.0\n",
    "    c = np.asarray(other_line.coords, float)\n",
    "    p0 = Point(float(c[0,0]), float(c[0,1]))\n",
    "    p1 = Point(float(c[-1,0]), float(c[-1,1]))\n",
    "    d0 = float(target_line.project(p0))\n",
    "    d1 = float(target_line.project(p1))\n",
    "    span = abs(d1 - d0)\n",
    "    return float(span / float(other_line.length))\n",
    "\n",
    "\n",
    "def merge_parallel_duplicate_edges(nodes, edges, next_node_id, cost_ctx,\n",
    "                                  parallel_max_dist, parallel_max_angle_deg,\n",
    "                                  parallel_min_length, max_merges,\n",
    "                                  overlap_ratio_min=0.60):\n",
    "    \"\"\"\n",
    "    True merge of near-parallel duplicate edges:\n",
    "      - identify candidate edge pairs (near + parallel + sufficient overlap)\n",
    "      - choose a keeper edge (longer)\n",
    "      - reroute all topology incident to the duplicate's endpoints onto the keeper by:\n",
    "          * splitting keeper at nearest points to duplicate endpoints (node inserted)\n",
    "          * rewiring incident edges from duplicate endpoints to those new nodes\n",
    "      - delete the duplicate edge and drop orphan nodes\n",
    "\n",
    "    Costs:\n",
    "      - if cost_ctx exists: duplicates have cost_sum; keeper gets averaged cost density\n",
    "      - if keeper is split during merging, propagate averaged cost by segment length\n",
    "    \"\"\"\n",
    "    # Build eligible list\n",
    "    eligible = [i for i, e in enumerate(edges)\n",
    "                if e is not None and not e.get(\"_deleted\", False)\n",
    "                and float(e.get(\"length\", e[\"geometry\"].length)) >= float(parallel_min_length)]\n",
    "\n",
    "    if not eligible:\n",
    "        return 0, next_node_id\n",
    "\n",
    "    geoms = [edges[i][\"geometry\"] for i in eligible]\n",
    "    tree = STRtree(geoms)\n",
    "\n",
    "    # map signature -> edge index (works even if STRtree returns geometry copies)\n",
    "    sig_to_eidx = {}\n",
    "    for i in eligible:\n",
    "        sig_to_eidx[_edge_signature(edges[i])] = i\n",
    "\n",
    "    idx_round = build_rounding_index(nodes, nd=6)  # for node reuse in split_edge_at_point\n",
    "\n",
    "    merges_done = 0\n",
    "    used_pairs = set()\n",
    "\n",
    "    for base_idx, ei in enumerate(eligible):\n",
    "        if merges_done >= int(max_merges):\n",
    "            break\n",
    "        e1 = edges[ei]\n",
    "        if e1 is None or e1.get(\"_deleted\", False):\n",
    "            continue\n",
    "\n",
    "        g1 = e1[\"geometry\"]\n",
    "        if float(g1.length) < float(parallel_min_length):\n",
    "            continue\n",
    "        b1 = float(edge_bearing_mod180(g1))\n",
    "\n",
    "        # query spatially nearby edges\n",
    "        hits = tree.query(g1.buffer(float(parallel_max_dist)))\n",
    "\n",
    "        for h in hits:\n",
    "            if merges_done >= int(max_merges):\n",
    "                break\n",
    "\n",
    "            sig = _edge_signature({\"geometry\": h})\n",
    "            ej = sig_to_eidx.get(sig, None)\n",
    "            if ej is None or ej == ei:\n",
    "                continue\n",
    "\n",
    "            a, b = (ei, ej) if ei < ej else (ej, ei)\n",
    "            if (a, b) in used_pairs:\n",
    "                continue\n",
    "            used_pairs.add((a, b))\n",
    "\n",
    "            e2 = edges[ej]\n",
    "            if e2 is None or e2.get(\"_deleted\", False):\n",
    "                continue\n",
    "\n",
    "            g2 = e2[\"geometry\"]\n",
    "            if float(g2.length) < float(parallel_min_length):\n",
    "                continue\n",
    "            b2 = float(edge_bearing_mod180(g2))\n",
    "            if angle_diff_180(b1, b2) > float(parallel_max_angle_deg):\n",
    "                continue\n",
    "\n",
    "            # distance check via nearest points\n",
    "            p1, p2 = nearest_points(g1, g2)\n",
    "            if float(p1.distance(p2)) > float(parallel_max_dist):\n",
    "                continue\n",
    "\n",
    "            # overlap check (avoid merging lines that are just briefly close)\n",
    "            # project shorter onto longer\n",
    "            if g1.length >= g2.length:\n",
    "                overlap = _endpoint_overlap_ratio_on_target(g1, g2)\n",
    "            else:\n",
    "                overlap = _endpoint_overlap_ratio_on_target(g2, g1)\n",
    "            if overlap < float(overlap_ratio_min):\n",
    "                continue\n",
    "\n",
    "            # Decide keeper (longer)\n",
    "            if g1.length >= g2.length:\n",
    "                keep_i, dup_i = ei, ej\n",
    "            else:\n",
    "                keep_i, dup_i = ej, ei\n",
    "\n",
    "            keep = edges[keep_i]\n",
    "            dup  = edges[dup_i]\n",
    "            if keep is None or dup is None or keep.get(\"_deleted\", False) or dup.get(\"_deleted\", False):\n",
    "                continue\n",
    "\n",
    "            # ensure endpoints are coherent with node coords (important before splitting / rewiring)\n",
    "            _snap_edge_endpoints_to_node_coords(nodes, keep)\n",
    "            _snap_edge_endpoints_to_node_coords(nodes, dup)\n",
    "\n",
    "            keep_geom = keep[\"geometry\"]\n",
    "            dup_geom  = dup[\"geometry\"]\n",
    "\n",
    "            # --- averaged cost density (if enabled) ---\n",
    "            avg_cost_density = None\n",
    "            if cost_ctx is not None:\n",
    "                c_keep = float(keep.get(\"cost_sum\", 0.0))\n",
    "                c_dup  = float(dup.get(\"cost_sum\", 0.0))\n",
    "                avg_sum = 0.5 * (c_keep + c_dup)\n",
    "                Lk = float(keep_geom.length) if keep_geom.length else 1.0\n",
    "                avg_cost_density = avg_sum / Lk\n",
    "                # set keeper cost_sum to averaged (whole edge)\n",
    "                keep[\"cost_sum\"] = float(avg_sum)\n",
    "\n",
    "            # --- reroute duplicate endpoints onto keeper ---\n",
    "            # We handle each endpoint node of the duplicate edge:\n",
    "            #  - find nearest point on keeper geometry\n",
    "            #  - split keeper at that point (creates node)\n",
    "            #  - rewire all edges incident to that endpoint node to the new node\n",
    "            #  - then delete duplicate edge\n",
    "            dup_end_nodes = [int(dup[\"u\"]), int(dup[\"v\"])]\n",
    "\n",
    "            for old_n in dup_end_nodes:\n",
    "                # If old_n also lies on keeper already (shared node), skip\n",
    "                if old_n == int(keep[\"u\"]) or old_n == int(keep[\"v\"]):\n",
    "                    continue\n",
    "\n",
    "                old_xy = nodes.get(old_n, None)\n",
    "                if old_xy is None:\n",
    "                    continue\n",
    "\n",
    "                # nearest point on current keeper geometry\n",
    "                # (keeper may have been split already; we must pick the segment that is closest)\n",
    "                # Find the best segment among all edges that are part of the keeper \"cluster\".\n",
    "                # We approximate by selecting the single closest eligible edge to old_xy with similar bearing.\n",
    "                best_seg_i = None\n",
    "                best_dist = None\n",
    "                old_pt = Point(float(old_xy[0]), float(old_xy[1]))\n",
    "\n",
    "                # search among all current edges: those not deleted and within buffer\n",
    "                for ii, ee in enumerate(edges):\n",
    "                    if ee is None or ee.get(\"_deleted\", False):\n",
    "                        continue\n",
    "                    if float(ee.get(\"length\", ee[\"geometry\"].length)) < 1e-9:\n",
    "                        continue\n",
    "                    # use geometry distance as quick filter\n",
    "                    d = float(ee[\"geometry\"].distance(old_pt))\n",
    "                    if d > float(parallel_max_dist) * 2.0:  # generous; we only need the keeper vicinity\n",
    "                        continue\n",
    "                    if best_dist is None or d < best_dist:\n",
    "                        best_dist = d\n",
    "                        best_seg_i = ii\n",
    "\n",
    "                if best_seg_i is None:\n",
    "                    continue\n",
    "\n",
    "                # Split that segment at nearest point\n",
    "                seg = edges[best_seg_i]\n",
    "                proj_pt = seg[\"geometry\"].interpolate(seg[\"geometry\"].project(old_pt))\n",
    "                parent_cost_before = seg.get(\"cost_sum\", None)\n",
    "\n",
    "                new_n, next_node_id, did_split = split_edge_at_point(\n",
    "                    nodes, idx_round, edges, best_seg_i, proj_pt, next_node_id,\n",
    "                    cost_ctx=cost_ctx, parent_cost=parent_cost_before\n",
    "                )\n",
    "\n",
    "                # If we have averaged cost density, enforce it on the affected segment(s)\n",
    "                if avg_cost_density is not None:\n",
    "                    # segment replaced in place\n",
    "                    edges[best_seg_i][\"cost_sum\"] = float(avg_cost_density * float(edges[best_seg_i][\"geometry\"].length))\n",
    "                    if did_split:\n",
    "                        # new segment appended at the end\n",
    "                        edges[-1][\"cost_sum\"] = float(avg_cost_density * float(edges[-1][\"geometry\"].length))\n",
    "\n",
    "                # rewire incident edges (except the duplicate edge itself) from old_n -> new_n\n",
    "                _reroute_incident_edges(nodes, edges, old_node=old_n, new_node=new_n,\n",
    "                                        exclude_edge_idx=dup_i, cost_ctx=cost_ctx)\n",
    "\n",
    "                # old node might become orphan later; we clean at the end\n",
    "\n",
    "            # delete the duplicate edge itself\n",
    "            dup[\"_deleted\"] = True\n",
    "\n",
    "            merges_done += 1\n",
    "\n",
    "    # cleanup: remove orphan nodes (degree==0) and drop deleted edges\n",
    "    deg = {}\n",
    "    for e in edges:\n",
    "        if e is None or e.get(\"_deleted\", False):\n",
    "            continue\n",
    "        deg[int(e[\"u\"])] = deg.get(int(e[\"u\"]), 0) + 1\n",
    "        deg[int(e[\"v\"])] = deg.get(int(e[\"v\"]), 0) + 1\n",
    "\n",
    "    orphans = [nid for nid in list(nodes.keys()) if deg.get(int(nid), 0) == 0]\n",
    "    for nid in orphans:\n",
    "        nodes.pop(int(nid), None)\n",
    "\n",
    "    return merges_done, next_node_id\n",
    "\n",
    "\n",
    "# ---------------------- annotate directionality ------------------------------\n",
    "def add_directionality(edges, slope_raster_path, convention, is_downhill):\n",
    "    \"\"\"\n",
    "    Adds fields:\n",
    "      - slope_dir_deg (converted to cw_from_north)\n",
    "      - from, to (node ids)\n",
    "    \"\"\"\n",
    "    if not slope_raster_path:\n",
    "        return\n",
    "\n",
    "    with rasterio.open(slope_raster_path) as src:\n",
    "        for e in edges:\n",
    "            geom = e[\"geometry\"]\n",
    "            mid = geom.interpolate(0.5, normalized=True)\n",
    "            sraw = raster_value_at_xy(src, (mid.x, mid.y))\n",
    "            sdir = convert_slope_dir_to_cw_from_north(sraw, convention)\n",
    "            if np.isfinite(sdir) and not bool(is_downhill):\n",
    "                sdir = (sdir + 180.0) % 360.0  # raster indicates uphill; convert to downhill\n",
    "\n",
    "            e[\"slope_dir_deg\"] = float(sdir) if np.isfinite(sdir) else np.nan\n",
    "\n",
    "            # choose orientation closest to downhill direction\n",
    "            coords = np.asarray(geom.coords, float)\n",
    "            b_uv = bearing_cw_from_north(coords[0], coords[-1])\n",
    "            b_vu = (b_uv + 180.0) % 360.0\n",
    "\n",
    "            if not np.isfinite(sdir):\n",
    "                e[\"from\"] = int(e[\"u\"]); e[\"to\"] = int(e[\"v\"])\n",
    "                continue\n",
    "\n",
    "            d_uv = abs(((b_uv - sdir + 180.0) % 360.0) - 180.0)\n",
    "            d_vu = abs(((b_vu - sdir + 180.0) % 360.0) - 180.0)\n",
    "            if d_uv <= d_vu:\n",
    "                e[\"from\"] = int(e[\"u\"]); e[\"to\"] = int(e[\"v\"])\n",
    "            else:\n",
    "                e[\"from\"] = int(e[\"v\"]); e[\"to\"] = int(e[\"u\"])\n",
    "\n",
    "\n",
    "# ============================ PIPELINE ======================================\n",
    "\n",
    "# ---- 1) Read skeleton raster ----\n",
    "with rasterio.open(in_skeleton) as src:\n",
    "    skel = src.read(1, masked=True)\n",
    "    transform = src.transform\n",
    "    crs = src.crs\n",
    "\n",
    "skel_bool = (np.asarray(skel.filled(0) if np.ma.isMaskedArray(skel) else skel) > 0).astype(np.uint8)\n",
    "\n",
    "# ---- 2) Build sknw graph from skeleton ----\n",
    "G = sknw.build_sknw(skel_bool, multi=True)\n",
    "\n",
    "# ---- 3) Auto simplify tolerance if not provided ----\n",
    "if simplify_tol is None:\n",
    "    px = pixel_size_from_transform(transform)\n",
    "    simplify_tol = 0.5 * px  # good default for staircase reduction\n",
    "\n",
    "# ---- 4) Export nodes + edges (split at junctions) ----\n",
    "nodes_gdf, edges_gdf = build_nodes_edges_gdfs(G, transform, crs, simplify_tol, smooth_iters)\n",
    "\n",
    "# ---- 5) Contract degree-2 nodes to create analysis network ----\n",
    "nodes2_gdf, edges2_gdf = contract_degree2_graph(G, nodes_gdf, edges_gdf)\n",
    "\n",
    "# ---- 6) Convert to mutable in-memory network (dict/list) ----\n",
    "nodes = build_node_dict(nodes2_gdf)\n",
    "edges = []\n",
    "for r in edges2_gdf.itertuples(index=False):\n",
    "    edges.append({\n",
    "        \"u\": int(r.u),\n",
    "        \"v\": int(r.v),\n",
    "        \"n_segs\": int(getattr(r, \"n_segs\", 1)),\n",
    "        \"length\": float(r.length),\n",
    "        \"is_bridge\": bool(getattr(r, \"is_bridge\", False)),\n",
    "        \"bridge_type\": str(getattr(r, \"bridge_type\", \"\")),\n",
    "        \"geometry\": r.geometry\n",
    "    })\n",
    "\n",
    "next_node_id = int(max(nodes.keys())) + 1 if nodes else 0\n",
    "\n",
    "# ---- 7) Optional costs on current edges (needed for later avg-parent metrics) ----\n",
    "cost_ctx = None\n",
    "if cost_raster:\n",
    "    with rasterio.open(cost_raster) as cs:\n",
    "        carr = cs.read(1)\n",
    "        cnod = cs.nodata\n",
    "        ctr  = cs.transform\n",
    "        if cost_sampling_step is None:\n",
    "            cpx = pixel_size_from_transform(ctr)\n",
    "            cstep = 0.5 * cpx\n",
    "        else:\n",
    "            cstep = float(cost_sampling_step)\n",
    "\n",
    "        cost_ctx = {\n",
    "            \"arr\": carr,\n",
    "            \"transform\": ctr,\n",
    "            \"nodata\": cnod,\n",
    "            \"step\": float(cstep),\n",
    "            \"nodata_to_nan\": bool(cost_nodata_to_nan),\n",
    "            \"unique_cells\": bool(cost_unique_cells),\n",
    "        }\n",
    "\n",
    "    for e in edges:\n",
    "        e[\"cost_sum\"] = cost_sum_along_line(\n",
    "            e[\"geometry\"], cost_ctx[\"arr\"], cost_ctx[\"transform\"], cost_ctx[\"nodata\"], cost_ctx[\"step\"],\n",
    "            nodata_to_nan=cost_ctx[\"nodata_to_nan\"], unique_cells=cost_ctx[\"unique_cells\"]\n",
    "        )\n",
    "\n",
    "# ---- 8) Post-contraction gap filling (multiple rounds) ----\n",
    "total_ep_bridges = 0\n",
    "total_vertex_snaps = 0\n",
    "\n",
    "for round_i in range(int(max_bridge_rounds)):\n",
    "    # endpoint↔endpoint bridging\n",
    "    if bridge_endpoints:\n",
    "        added = bridge_endpoints_round(\n",
    "            nodes, edges,\n",
    "            max_gap_dist=max_gap_dist,\n",
    "            max_angle_deg=max_angle_deg,\n",
    "            direction_step=direction_step,\n",
    "            max_bridges_per_endpoint=max_bridges_per_endpoint,\n",
    "            bridge_type=f\"endpoint_endpoint_r{round_i+1}\"\n",
    "        )\n",
    "        total_ep_bridges += added\n",
    "\n",
    "        # compute costs for newly added bridge edges\n",
    "        if cost_ctx is not None and added > 0:\n",
    "            for e in edges[-added:]:\n",
    "                e[\"cost_sum\"] = cost_sum_along_line(\n",
    "                    e[\"geometry\"], cost_ctx[\"arr\"], cost_ctx[\"transform\"], cost_ctx[\"nodata\"], cost_ctx[\"step\"],\n",
    "                    nodata_to_nan=cost_ctx[\"nodata_to_nan\"], unique_cells=cost_ctx[\"unique_cells\"]\n",
    "                )\n",
    "\n",
    "    # endpoint→vertex snapping\n",
    "    if bridge_to_vertices:\n",
    "        added_vs, next_node_id = snap_endpoints_to_vertices_round(\n",
    "            nodes, edges, next_node_id, cost_ctx,\n",
    "            max_snap_dist=max_snap_dist,\n",
    "            max_snap_angle_deg=max_snap_angle_deg,\n",
    "            max_target_angle_deg=max_target_angle_deg,\n",
    "            direction_step=direction_step,\n",
    "            max_snaps_per_endpoint=max_vertex_snaps_per_endpoint,\n",
    "            vertex_stride=vertex_candidate_stride\n",
    "        )\n",
    "        total_vertex_snaps += added_vs\n",
    "\n",
    "print(f\"[Gap filling] endpoint↔endpoint bridges added: {total_ep_bridges} | endpoint→vertex snaps added: {total_vertex_snaps}\")\n",
    "\n",
    "# ---- 9) Merge close parallel duplicates (TRUE MERGE) ----\n",
    "parallel_merged = 0\n",
    "if join_parallel_lines:\n",
    "    parallel_merged, next_node_id = merge_parallel_duplicate_edges(\n",
    "        nodes, edges, next_node_id, cost_ctx,\n",
    "        parallel_max_dist=parallel_max_dist,\n",
    "        parallel_max_angle_deg=parallel_max_angle_deg,\n",
    "        parallel_min_length=parallel_min_length,\n",
    "        max_merges=max_parallel_connectors,   \n",
    "        overlap_ratio_min=0.60                # internal guard; adjust if needed\n",
    "    )\n",
    "print(f\"[Parallel merge] merged duplicates: {parallel_merged}\" if join_parallel_lines else \"[Parallel merge] disabled\")\n",
    "\n",
    "# ---- 10) Optional directionality on final edges ----\n",
    "if slope_dir_raster:\n",
    "    add_directionality(\n",
    "        edges,\n",
    "        slope_raster_path=slope_dir_raster,\n",
    "        convention=slope_dir_convention,\n",
    "        is_downhill=slope_dir_is_downhill\n",
    "    )\n",
    "\n",
    "# ---- 11) Build final GeoDataFrames and export ----\n",
    "deg = recompute_node_degrees(nodes, edges)\n",
    "\n",
    "node_rows = []\n",
    "for nid, xy in nodes.items():\n",
    "    node_rows.append({\n",
    "        \"node\": int(nid),\n",
    "        \"degree\": int(deg.get(int(nid), 0)),\n",
    "        \"x\": float(xy[0]),\n",
    "        \"y\": float(xy[1]),\n",
    "        \"geometry\": Point(float(xy[0]), float(xy[1]))\n",
    "    })\n",
    "nodes_out = gpd.GeoDataFrame(node_rows, geometry=\"geometry\", crs=crs)\n",
    "\n",
    "edge_rows = []\n",
    "for i, e in enumerate(edges):\n",
    "    row = {\n",
    "        \"edge_id\": int(i),\n",
    "        \"u\": int(e[\"u\"]),\n",
    "        \"v\": int(e[\"v\"]),\n",
    "        \"length\": float(e.get(\"length\", e[\"geometry\"].length)),\n",
    "        \"n_segs\": int(e.get(\"n_segs\", 1)),\n",
    "        \"is_bridge\": bool(e.get(\"is_bridge\", False)),\n",
    "        \"bridge_type\": str(e.get(\"bridge_type\", \"\")),\n",
    "        \"geometry\": e[\"geometry\"]\n",
    "    }\n",
    "    if \"cost_sum\" in e:\n",
    "        row[\"cost_sum\"] = float(e[\"cost_sum\"])\n",
    "    if \"avg_parent_cost_sum\" in e:\n",
    "        row[\"avg_parent_cost_sum\"] = float(e[\"avg_parent_cost_sum\"])\n",
    "    if \"slope_dir_deg\" in e:\n",
    "        row[\"slope_dir_deg\"] = float(e[\"slope_dir_deg\"]) if np.isfinite(e[\"slope_dir_deg\"]) else np.nan\n",
    "    if \"from\" in e and \"to\" in e:\n",
    "        row[\"from\"] = int(e[\"from\"])\n",
    "        row[\"to\"] = int(e[\"to\"])\n",
    "    edge_rows.append(row)\n",
    "\n",
    "edges_out = gpd.GeoDataFrame(edge_rows, geometry=\"geometry\", crs=crs)\n",
    "\n",
    "# write\n",
    "if os.path.exists(out_gpkg):\n",
    "    os.remove(out_gpkg)\n",
    "nodes_out.to_file(out_gpkg, layer=\"nodes\", driver=\"GPKG\")\n",
    "edges_out.to_file(out_gpkg, layer=\"edges\", driver=\"GPKG\")\n",
    "\n",
    "print(f\"Wrote network to: {out_gpkg} (layers: nodes, edges)\")\n",
    "print(f\"Original sknw export: nodes={len(nodes_gdf)} edges={len(edges_gdf)}\")\n",
    "print(f\"Contracted:          nodes={len(nodes2_gdf)} edges={len(edges2_gdf)}\")\n",
    "print(f\"Final:              nodes={len(nodes_out)} edges={len(edges_out)}\")\n",
    "print(f\"simplify_tol={simplify_tol:.4g} smooth_iters={smooth_iters}\")\n",
    "if cost_raster:\n",
    "    print(f\"Costs enabled: cost_sum (unique_cells={cost_unique_cells}, step={cost_ctx['step']:.4g}, nodata_to_nan={cost_nodata_to_nan})\")\n",
    "if slope_dir_raster:\n",
    "    print(f\"Directionality enabled: convention={slope_dir_convention}, slope_dir_is_downhill={slope_dir_is_downhill} -> fields: slope_dir_deg, from, to\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing with engine: fiona\n",
      "Written: /home/hector/Documents/Nazarij/channels_50conPix_network_metrics.gpkg\n",
      "Layers overwritten in output: nodes, edges\n"
     ]
    }
   ],
   "source": [
    "# Add connectivity metrics directly into the existing nodes/edges tables:\n",
    "#  1) Read nodes+edges from an existing GPKG\n",
    "#  2) Compute metrics with NetworkX\n",
    "#  3) Write new GeoPackage file with metrics,`overwrite_in_place=True` to replace the original GPKG.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "\n",
    "# ---- USER PARAMS ----\n",
    "gpkg_in   = \"/home/hector/Documents/channels_50conPix_network.gpkg\"\n",
    "nodes_lyr = \"nodes\"\n",
    "edges_lyr = \"edges\"\n",
    "\n",
    "# Output handling\n",
    "gpkg_out = \"/home/hector/Documents/channels_50conPix_network_metrics.gpkg\"\n",
    "overwrite_in_place = False  # if True, replaces gpkg_in with gpkg_out at the end\n",
    "\n",
    "# Graph options\n",
    "directed    = False\n",
    "use_weights = True\n",
    "length_col  = \"length\"   # created from geometry if missing\n",
    "\n",
    "# Extra/slow metrics\n",
    "compute_current_flow = False            # OFF by default (avoid warnings + heavy computation)\n",
    "compute_local_edge_connectivity = False # can be expensive\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def _sanitize_path(p: str) -> Path:\n",
    "    p = str(p).strip()\n",
    "    p = re.sub(r\"^[cC]/home/\", \"/home/\", p)  # fix c/home typo\n",
    "    outp = Path(p).expanduser()\n",
    "    if not outp.is_absolute():\n",
    "        outp = (Path.cwd() / outp).resolve()\n",
    "    outp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return outp\n",
    "\n",
    "\n",
    "def _safe_unlink(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "    except Exception as e:\n",
    "        raise OSError(f\"Cannot remove existing file: {p}\\nClose it in QGIS/other apps and retry.\\n{e}\") from e\n",
    "\n",
    "\n",
    "# ---------- READ ----------\n",
    "gpkg_in_p = _sanitize_path(gpkg_in)\n",
    "gpkg_out_p = _sanitize_path(gpkg_out)\n",
    "\n",
    "nodes = gpd.read_file(gpkg_in_p.as_posix(), layer=nodes_lyr)\n",
    "edges = gpd.read_file(gpkg_in_p.as_posix(), layer=edges_lyr)\n",
    "\n",
    "if \"node\" not in nodes.columns:\n",
    "    raise ValueError(f\"'{nodes_lyr}' layer must contain a 'node' column.\")\n",
    "for c in (\"u\", \"v\"):\n",
    "    if c not in edges.columns:\n",
    "        raise ValueError(f\"'{edges_lyr}' layer must contain '{c}' columns.\")\n",
    "if \"geometry\" not in edges.columns:\n",
    "    raise ValueError(f\"'{edges_lyr}' layer must contain geometries.\")\n",
    "\n",
    "edges_out = edges.copy()\n",
    "if length_col not in edges_out.columns:\n",
    "    edges_out[length_col] = edges_out.geometry.length.astype(float)\n",
    "\n",
    "has_k = \"k\" in edges_out.columns\n",
    "\n",
    "# ---------- BUILD GRAPH ----------\n",
    "if directed:\n",
    "    G = nx.MultiDiGraph() if has_k else nx.DiGraph()\n",
    "    G_simple = nx.DiGraph()\n",
    "else:\n",
    "    G = nx.MultiGraph() if has_k else nx.Graph()\n",
    "    G_simple = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(nodes[\"node\"].astype(int).tolist())\n",
    "\n",
    "for r in edges_out.itertuples(index=False):\n",
    "    u = int(getattr(r, \"u\"))\n",
    "    v = int(getattr(r, \"v\"))\n",
    "    w = float(getattr(r, length_col))\n",
    "    if has_k:\n",
    "        k = int(getattr(r, \"k\"))\n",
    "        G.add_edge(u, v, key=k, **{length_col: w})\n",
    "    else:\n",
    "        G.add_edge(u, v, **{length_col: w})\n",
    "\n",
    "    # Simple view: keep minimum weight between u-v\n",
    "    if G_simple.has_edge(u, v):\n",
    "        if w < G_simple[u][v].get(length_col, np.inf):\n",
    "            G_simple[u][v][length_col] = w\n",
    "    else:\n",
    "        G_simple.add_edge(u, v, **{length_col: w})\n",
    "\n",
    "node_ids = nodes[\"node\"].astype(int)\n",
    "\n",
    "# ---------- NODE METRICS ----------\n",
    "degree = dict(G.degree())\n",
    "strength = dict(G.degree(weight=length_col)) if use_weights else {n: np.nan for n in G.nodes()}\n",
    "\n",
    "betweenness = nx.betweenness_centrality(G, weight=(length_col if use_weights else None), normalized=True)\n",
    "closeness   = nx.closeness_centrality(G, distance=(length_col if use_weights else None))\n",
    "\n",
    "try:\n",
    "    eigenvector = nx.eigenvector_centrality_numpy(G, weight=None)\n",
    "except Exception:\n",
    "    eigenvector = nx.eigenvector_centrality(G, max_iter=2000, tol=1e-8, weight=None)\n",
    "\n",
    "harmonic = nx.harmonic_centrality(G, distance=(length_col if use_weights else None))\n",
    "\n",
    "try:\n",
    "    pagerank = nx.pagerank(G, weight=(length_col if use_weights else None))\n",
    "except Exception:\n",
    "    pagerank = {n: np.nan for n in G.nodes()}\n",
    "\n",
    "# clustering/core best on undirected view\n",
    "try:\n",
    "    clustering = nx.clustering(G_simple if not directed else nx.Graph(G_simple))\n",
    "except Exception:\n",
    "    clustering = {n: np.nan for n in G.nodes()}\n",
    "\n",
    "try:\n",
    "    core = nx.core_number(G_simple if not directed else nx.Graph(G_simple))\n",
    "except Exception:\n",
    "    core = {n: np.nan for n in G.nodes()}\n",
    "\n",
    "# components\n",
    "if directed:\n",
    "    comps = list(nx.weakly_connected_components(G))\n",
    "else:\n",
    "    comps = list(nx.connected_components(G))\n",
    "comp_id, comp_size = {}, {}\n",
    "for i, comp in enumerate(comps):\n",
    "    s = len(comp)\n",
    "    for n in comp:\n",
    "        comp_id[n] = i\n",
    "        comp_size[n] = s\n",
    "\n",
    "nodes2 = nodes.copy()\n",
    "nodes2[\"degree\"]      = node_ids.map(degree).astype(float)\n",
    "nodes2[\"strength\"]    = node_ids.map(strength).astype(float)\n",
    "nodes2[\"betweenness\"] = node_ids.map(betweenness).astype(float)\n",
    "nodes2[\"closeness\"]   = node_ids.map(closeness).astype(float)\n",
    "nodes2[\"eigenvector\"] = node_ids.map(eigenvector).astype(float)\n",
    "\n",
    "nodes2[\"harmonic\"]    = node_ids.map(harmonic).astype(float)\n",
    "nodes2[\"pagerank\"]    = node_ids.map(pagerank).astype(float)\n",
    "nodes2[\"clustering\"]  = node_ids.map(clustering).astype(float)\n",
    "nodes2[\"core\"]        = node_ids.map(core).astype(float)\n",
    "nodes2[\"component\"]   = node_ids.map(comp_id).astype(int)\n",
    "nodes2[\"comp_size\"]   = node_ids.map(comp_size).astype(int)\n",
    "\n",
    "if directed:\n",
    "    indeg  = dict(G.in_degree())\n",
    "    outdeg = dict(G.out_degree())\n",
    "    nodes2[\"in_degree\"]  = node_ids.map(indeg).astype(float)\n",
    "    nodes2[\"out_degree\"] = node_ids.map(outdeg).astype(float)\n",
    "\n",
    "# ---------- EDGE METRICS (emphasis) ----------\n",
    "deg_map = nodes2.set_index(\"node\")[\"degree\"].to_dict()\n",
    "bet_map = nodes2.set_index(\"node\")[\"betweenness\"].to_dict()\n",
    "clo_map = nodes2.set_index(\"node\")[\"closeness\"].to_dict()\n",
    "eig_map = nodes2.set_index(\"node\")[\"eigenvector\"].to_dict()\n",
    "\n",
    "edges2 = edges_out.copy()\n",
    "\n",
    "# Endpoint metrics\n",
    "edges2[\"u_degree\"] = edges2[\"u\"].astype(int).map(deg_map).astype(float)\n",
    "edges2[\"v_degree\"] = edges2[\"v\"].astype(int).map(deg_map).astype(float)\n",
    "edges2[\"u_betweenness\"] = edges2[\"u\"].astype(int).map(bet_map).astype(float)\n",
    "edges2[\"v_betweenness\"] = edges2[\"v\"].astype(int).map(bet_map).astype(float)\n",
    "edges2[\"u_closeness\"] = edges2[\"u\"].astype(int).map(clo_map).astype(float)\n",
    "edges2[\"v_closeness\"] = edges2[\"v\"].astype(int).map(clo_map).astype(float)\n",
    "edges2[\"u_eigenvector\"] = edges2[\"u\"].astype(int).map(eig_map).astype(float)\n",
    "edges2[\"v_eigenvector\"] = edges2[\"v\"].astype(int).map(eig_map).astype(float)\n",
    "\n",
    "edges2[\"deg_sum\"]  = edges2[\"u_degree\"] + edges2[\"v_degree\"]\n",
    "edges2[\"deg_diff\"] = (edges2[\"u_degree\"] - edges2[\"v_degree\"]).abs()\n",
    "\n",
    "# Edge betweenness (kept)\n",
    "edge_betw = nx.edge_betweenness_centrality(G, weight=(length_col if use_weights else None), normalized=True)\n",
    "\n",
    "def _edge_betw_lookup(u, v, k=None):\n",
    "    if has_k:\n",
    "        if (u, v, k) in edge_betw: return edge_betw[(u, v, k)]\n",
    "        if (v, u, k) in edge_betw and not directed: return edge_betw[(v, u, k)]\n",
    "        return np.nan\n",
    "    else:\n",
    "        if (u, v) in edge_betw: return edge_betw[(u, v)]\n",
    "        if (v, u) in edge_betw and not directed: return edge_betw[(v, u)]\n",
    "        return np.nan\n",
    "\n",
    "if has_k:\n",
    "    edges2[\"edge_betweenness\"] = [\n",
    "        float(_edge_betw_lookup(int(u), int(v), int(k)))\n",
    "        for u, v, k in zip(edges2[\"u\"], edges2[\"v\"], edges2[\"k\"])\n",
    "    ]\n",
    "else:\n",
    "    edges2[\"edge_betweenness\"] = [\n",
    "        float(_edge_betw_lookup(int(u), int(v)))\n",
    "        for u, v in zip(edges2[\"u\"], edges2[\"v\"])\n",
    "    ]\n",
    "\n",
    "# Bridges (cut edges) on undirected simple graph\n",
    "if not directed:\n",
    "    try:\n",
    "        bridge_pairs = set(tuple(sorted(e)) for e in nx.bridges(G_simple))\n",
    "    except Exception:\n",
    "        bridge_pairs = set()\n",
    "\n",
    "    if has_k:\n",
    "        # If parallel edges exist between u-v, it cannot be a bridge\n",
    "        multiplicity = {}\n",
    "        for u, v, k in G.edges(keys=True):\n",
    "            a, b = (u, v) if u <= v else (v, u)\n",
    "            multiplicity[(a, b)] = multiplicity.get((a, b), 0) + 1\n",
    "\n",
    "        edges2[\"is_bridge\"] = [\n",
    "            bool((tuple(sorted((int(u), int(v)))) in bridge_pairs) and (multiplicity.get(tuple(sorted((int(u), int(v)))), 0) == 1))\n",
    "            for u, v in zip(edges2[\"u\"], edges2[\"v\"])\n",
    "        ]\n",
    "    else:\n",
    "        edges2[\"is_bridge\"] = [bool(tuple(sorted((int(u), int(v)))) in bridge_pairs) for u, v in zip(edges2[\"u\"], edges2[\"v\"])]\n",
    "else:\n",
    "    edges2[\"is_bridge\"] = False\n",
    "\n",
    "# Geometry-based edge measures\n",
    "def _straight_dist(geom):\n",
    "    if geom is None or geom.is_empty:\n",
    "        return np.nan\n",
    "    coords = np.asarray(geom.coords)\n",
    "    if coords.shape[0] < 2:\n",
    "        return np.nan\n",
    "    return float(np.linalg.norm(coords[-1] - coords[0]))\n",
    "\n",
    "edges2[\"straight_dist\"] = edges2.geometry.apply(_straight_dist).astype(float)\n",
    "edges2[\"sinuosity\"] = (edges2[length_col] / edges2[\"straight_dist\"]).replace([np.inf, -np.inf], np.nan)\n",
    "edges2[\"straightness\"] = (edges2[\"straight_dist\"] / edges2[length_col]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Triangle-based edge measures (undirected)\n",
    "def _common_neighbors(u, v):\n",
    "    try:\n",
    "        return len(list(nx.common_neighbors(G_simple, int(u), int(v))))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "if not directed:\n",
    "    tris = [_common_neighbors(u, v) for u, v in zip(edges2[\"u\"], edges2[\"v\"])]\n",
    "    edges2[\"edge_triangles\"] = np.asarray(tris, dtype=float)\n",
    "\n",
    "    ecc = []\n",
    "    for u, v, tri in zip(edges2[\"u\"], edges2[\"v\"], edges2[\"edge_triangles\"]):\n",
    "        du = G_simple.degree(int(u))\n",
    "        dv = G_simple.degree(int(v))\n",
    "        denom = min(du - 1, dv - 1)\n",
    "        ecc.append(float(tri / denom) if denom > 0 else 0.0)\n",
    "    edges2[\"edge_clustering\"] = np.asarray(ecc, dtype=float)\n",
    "else:\n",
    "    edges2[\"edge_triangles\"] = np.nan\n",
    "    edges2[\"edge_clustering\"] = np.nan\n",
    "\n",
    "# Local edge connectivity (optional)\n",
    "def _local_ec(u, v):\n",
    "    if not compute_local_edge_connectivity:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(nx.local_edge_connectivity(G_simple, int(u), int(v)))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "edges2[\"local_edge_connectivity\"] = [_local_ec(u, v) for u, v in zip(edges2[\"u\"], edges2[\"v\"])]\n",
    "\n",
    "# Current-flow edge betweenness (optional; often fragile/slow)\n",
    "edges2[\"edge_current_flow_betweenness\"] = np.nan\n",
    "if (not directed) and compute_current_flow:\n",
    "    edge_cf = {}\n",
    "    for comp in nx.connected_components(G_simple):\n",
    "        H = G_simple.subgraph(comp).copy()\n",
    "        if H.number_of_nodes() < 3 or H.number_of_edges() < 2:\n",
    "            continue\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "            try:\n",
    "                part = nx.edge_current_flow_betweenness_centrality(\n",
    "                    H, weight=(length_col if use_weights else None), normalized=True\n",
    "                )\n",
    "                for e, val in part.items():\n",
    "                    edge_cf[e] = float(val) if np.isfinite(val) else np.nan\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    def _cf_lookup(u, v):\n",
    "        if (u, v) in edge_cf: return edge_cf[(u, v)]\n",
    "        if (v, u) in edge_cf: return edge_cf[(v, u)]\n",
    "        return np.nan\n",
    "\n",
    "    edges2[\"edge_current_flow_betweenness\"] = [float(_cf_lookup(int(u), int(v))) for u, v in zip(edges2[\"u\"], edges2[\"v\"])]\n",
    "\n",
    "# ---------- WRITE ----------\n",
    "# This avoids Fiona \"append\" bugs: recreate the file and write BOTH layers in one go.\n",
    "_safe_unlink(gpkg_out_p)\n",
    "\n",
    "# Write nodes (mode=\"w\") and edges (mode=\"w\") to distinct layers by using separate files? No:\n",
    "# For GPKG, writing a second layer usually implies append. Fiona is the flaky part.\n",
    "# Workaround: use pyogrio if available; otherwise write separate temporary GPKGs then merge is messy.\n",
    "#\n",
    "# Practical robust approach in many conda setups: tell GeoPandas to use the \"pyogrio\" engine if installed.\n",
    "engine = \"pyogrio\"\n",
    "try:\n",
    "    import pyogrio  # noqa: F401\n",
    "except Exception:\n",
    "    engine = \"fiona\"\n",
    "\n",
    "print(f\"Writing with engine: {engine}\")\n",
    "nodes2.to_file(gpkg_out_p.as_posix(), layer=nodes_lyr, driver=\"GPKG\", engine=engine)\n",
    "edges2.to_file(gpkg_out_p.as_posix(), layer=edges_lyr, driver=\"GPKG\", engine=engine)\n",
    "\n",
    "print(f\"Written: {gpkg_out_p}\")\n",
    "print(f\"Layers overwritten in output: {nodes_lyr}, {edges_lyr}\")\n",
    "\n",
    "# Optional: replace input file (only if everything succeeded)\n",
    "if overwrite_in_place:\n",
    "    # Ensure input is not open elsewhere\n",
    "    tmp_backup = gpkg_in_p.with_suffix(\".backup.gpkg\")\n",
    "    try:\n",
    "        if tmp_backup.exists():\n",
    "            tmp_backup.unlink()\n",
    "        gpkg_in_p.rename(tmp_backup)\n",
    "        gpkg_out_p.rename(gpkg_in_p)\n",
    "        tmp_backup.unlink(missing_ok=True)\n",
    "        print(f\"Replaced original GPKG in-place: {gpkg_in_p}\")\n",
    "    except Exception as e:\n",
    "        raise OSError(\n",
    "            \"Failed to replace original file. The output GPKG is still available.\\n\"\n",
    "            f\"Output: {gpkg_out_p}\\nBackup (if created): {tmp_backup}\\n{e}\"\n",
    "        ) from e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
